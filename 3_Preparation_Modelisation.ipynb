{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Préparation pour la Modélisation\n",
    "\n",
    "Ce notebook se concentre sur la troisième étape du processus d'analyse prédictive : la préparation des données pour la modélisation. Nous allons séparer les données en ensembles d'entraînement et de test, standardiser les variables numériques, et sauvegarder les données préparées pour les utiliser dans les notebooks de modélisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Importation des bibliothèques nécessaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importation des bibliothèques\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import joblib\n",
    "\n",
    "# Bibliothèques pour la préparation des données\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Configuration pour les visualisations\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "# Pour afficher toutes les colonnes\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Pour une meilleure lisibilité des graphiques\n",
    "%matplotlib inline\n",
    "\n",
    "# Configuration de l'aléatoire pour la reproductibilité\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Chargement des données préparées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chemin vers les données préparées\n",
    "data_dir = \"/home/ubuntu/notebooks/data\"\n",
    "data_path = os.path.join(data_dir, \"credit_data_prepared.pkl\")\n",
    "\n",
    "# Vérification de l'existence du fichier\n",
    "if os.path.exists(data_path):\n",
    "    print(f\"Le fichier existe à l'emplacement : {data_path}\")\n",
    "    # Chargement des données\n",
    "    df = pd.read_pickle(data_path)\n",
    "    print(f\"Données chargées avec succès. Dimensions : {df.shape[0]} lignes x {df.shape[1]} colonnes\")\n",
    "else:\n",
    "    print(f\"Erreur : Le fichier n'existe pas à l'emplacement : {data_path}\")\n",
    "    # Si le fichier n'existe pas, on charge les données brutes et on applique les transformations\n",
    "    print(\"Tentative de chargement des données brutes...\")\n",
    "    excel_path = \"/home/ubuntu/upload/ab4e5df7-08a5-4e76-8ddd-9d4f845ecff1.xlsx\"\n",
    "    if os.path.exists(excel_path):\n",
    "        df = pd.read_excel(excel_path)\n",
    "        # Renommage des colonnes\n",
    "        colonnes_renommees = {\n",
    "            \"client's age\": \"age\",\n",
    "            \"marital status\": \"marital_status\",\n",
    "            \"amount of expenses\": \"expenses\",\n",
    "            \"amount of income\": \"income\",\n",
    "            \"amount requested of loan\": \"loan_amount\",\n",
    "            \"price of good\": \"good_price\",\n",
    "            \"credit status\": \"credit_status\"\n",
    "        }\n",
    "        df = df.rename(columns=colonnes_renommees)\n",
    "        # Transformation de la variable cible\n",
    "        df['credit_status'] = df['credit_status'].map({\"Yes\": 0, \"No\": 1})\n",
    "        print(f\"Données brutes chargées et transformées. Dimensions : {df.shape[0]} lignes x {df.shape[1]} colonnes\")\n",
    "    else:\n",
    "        print(f\"Erreur : Le fichier de données brutes n'existe pas non plus à l'emplacement : {excel_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Aperçu des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affichage des premières lignes\n",
    "print(\"Aperçu des 5 premières lignes :\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Informations sur les colonnes\n",
    "print(\"Informations sur les colonnes :\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des valeurs manquantes\n",
    "print(\"Vérification des valeurs manquantes :\")\n",
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Ingénierie des caractéristiques\n",
    "\n",
    "Basé sur l'analyse exploratoire, nous allons créer quelques caractéristiques supplémentaires qui pourraient être utiles pour la modélisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du ratio prêt/revenu\n",
    "df['loan_to_income'] = df['loan_amount'] / df['income']\n",
    "\n",
    "# Création du ratio prix du bien/prêt\n",
    "df['good_price_to_loan'] = df['good_price'] / df['loan_amount']\n",
    "\n",
    "# Création du ratio dépenses/revenu\n",
    "df['expenses_to_income'] = df['expenses'] / df['income']\n",
    "\n",
    "# Affichage des nouvelles caractéristiques\n",
    "print(\"Aperçu des données avec les nouvelles caractéristiques :\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives des nouvelles caractéristiques\n",
    "print(\"Statistiques descriptives des nouvelles caractéristiques :\")\n",
    "df[['loan_to_income', 'good_price_to_loan', 'expenses_to_income']].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des valeurs infinies ou NaN dans les nouvelles caractéristiques\n",
    "print(\"Vérification des valeurs infinies ou NaN dans les nouvelles caractéristiques :\")\n",
    "print(f\"Nombre de valeurs infinies dans loan_to_income : {np.isinf(df['loan_to_income']).sum()}\")\n",
    "print(f\"Nombre de valeurs NaN dans loan_to_income : {np.isnan(df['loan_to_income']).sum()}\")\n",
    "print(f\"Nombre de valeurs infinies dans good_price_to_loan : {np.isinf(df['good_price_to_loan']).sum()}\")\n",
    "print(f\"Nombre de valeurs NaN dans good_price_to_loan : {np.isnan(df['good_price_to_loan']).sum()}\")\n",
    "print(f\"Nombre de valeurs infinies dans expenses_to_income : {np.isinf(df['expenses_to_income']).sum()}\")\n",
    "print(f\"Nombre de valeurs NaN dans expenses_to_income : {np.isnan(df['expenses_to_income']).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traitement des valeurs infinies ou NaN si nécessaire\n",
    "# Remplacement des valeurs infinies par la valeur maximale non infinie\n",
    "for col in ['loan_to_income', 'good_price_to_loan', 'expenses_to_income']:\n",
    "    if np.isinf(df[col]).sum() > 0 or np.isnan(df[col]).sum() > 0:\n",
    "        # Calcul de la valeur maximale non infinie et non NaN\n",
    "        max_val = df[col][~np.isinf(df[col]) & ~np.isnan(df[col])].max()\n",
    "        # Remplacement des valeurs infinies par la valeur maximale\n",
    "        df[col] = df[col].replace([np.inf, -np.inf], max_val)\n",
    "        # Remplacement des valeurs NaN par la médiane\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "        print(f\"Valeurs infinies et NaN remplacées dans {col}\")\n",
    "\n",
    "# Vérification après traitement\n",
    "print(\"\\nVérification après traitement :\")\n",
    "print(f\"Nombre de valeurs infinies dans loan_to_income : {np.isinf(df['loan_to_income']).sum()}\")\n",
    "print(f\"Nombre de valeurs NaN dans loan_to_income : {np.isnan(df['loan_to_income']).sum()}\")\n",
    "print(f\"Nombre de valeurs infinies dans good_price_to_loan : {np.isinf(df['good_price_to_loan']).sum()}\")\n",
    "print(f\"Nombre de valeurs NaN dans good_price_to_loan : {np.isnan(df['good_price_to_loan']).sum()}\")\n",
    "print(f\"Nombre de valeurs infinies dans expenses_to_income : {np.isinf(df['expenses_to_income']).sum()}\")\n",
    "print(f\"Nombre de valeurs NaN dans expenses_to_income : {np.isnan(df['expenses_to_income']).sum()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Séparation des données\n",
    "\n",
    "Nous allons maintenant séparer les données en variables explicatives (X) et variable cible (y), puis en ensembles d'entraînement et de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Définition des variables explicatives et de la variable cible\n",
    "# Nous incluons toutes les variables sauf credit_status\n",
    "X = df.drop('credit_status', axis=1)\n",
    "y = df['credit_status']\n",
    "\n",
    "# Affichage des dimensions\n",
    "print(f\"Dimensions de X : {X.shape}\")\n",
    "print(f\"Dimensions de y : {y.shape}\")\n",
    "\n",
    "# Affichage des noms des variables explicatives\n",
    "print(\"\\nVariables explicatives :\")\n",
    "print(X.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Séparation en ensembles d'entraînement et de test (80% entraînement, 20% test)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y)\n",
    "\n",
    "# Affichage des dimensions\n",
    "print(f\"Dimensions de X_train : {X_train.shape}\")\n",
    "print(f\"Dimensions de X_test : {X_test.shape}\")\n",
    "print(f\"Dimensions de y_train : {y_train.shape}\")\n",
    "print(f\"Dimensions de y_test : {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification de la distribution de la variable cible dans les ensembles d'entraînement et de test\n",
    "print(\"Distribution de la variable cible dans l'ensemble d'entraînement :\")\n",
    "print(y_train.value_counts(normalize=True) * 100)\n",
    "\n",
    "print(\"\\nDistribution de la variable cible dans l'ensemble de test :\")\n",
    "print(y_test.value_counts(normalize=True) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Standardisation des variables numériques\n",
    "\n",
    "Nous allons standardiser les variables numériques pour qu'elles aient une moyenne de 0 et un écart-type de 1. Cela est important pour de nombreux algorithmes d'apprentissage automatique, en particulier ceux qui sont basés sur la distance comme KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Ajustement du scaler sur l'ensemble d'entraînement uniquement\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Transformation des ensembles d'entraînement et de test\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Affichage des dimensions\n",
    "print(f\"Dimensions de X_train_scaled : {X_train_scaled.shape}\")\n",
    "print(f\"Dimensions de X_test_scaled : {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des arrays numpy en DataFrames pour une meilleure lisibilité\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, columns=X_test.columns, index=X_test.index)\n",
    "\n",
    "# Affichage des premières lignes des données standardisées\n",
    "print(\"Aperçu des données standardisées (ensemble d'entraînement) :\")\n",
    "X_train_scaled_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification des statistiques des données standardisées\n",
    "print(\"Statistiques des données standardisées (ensemble d'entraînement) :\")\n",
    "X_train_scaled_df.describe().T[['mean', 'std', 'min', 'max']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7 Sauvegarde des données préparées pour la modélisation\n",
    "\n",
    "Nous allons sauvegarder les ensembles d'entraînement et de test, ainsi que le scaler, pour les utiliser dans les notebooks de modélisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un dossier pour les données de modélisation\n",
    "model_data_dir = \"/home/ubuntu/notebooks/model_data\"\n",
    "os.makedirs(model_data_dir, exist_ok=True)\n",
    "\n",
    "# Sauvegarde des ensembles d'entraînement et de test\n",
    "joblib.dump(X_train, os.path.join(model_data_dir, \"X_train.pkl\"))\n",
    "joblib.dump(X_test, os.path.join(model_data_dir, \"X_test.pkl\"))\n",
    "joblib.dump(y_train, os.path.join(model_data_dir, \"y_train.pkl\"))\n",
    "joblib.dump(y_test, os.path.join(model_data_dir, \"y_test.pkl\"))\n",
    "\n",
    "# Sauvegarde des ensembles standardisés\n",
    "joblib.dump(X_train_scaled, os.path.join(model_data_dir, \"X_train_scaled.pkl\"))\n",
    "joblib.dump(X_test_scaled, os.path.join(model_data_dir, \"X_test_scaled.pkl\"))\n",
    "\n",
    "# Sauvegarde du scaler\n",
    "joblib.dump(scaler, os.path.join(model_data_dir, \"scaler.pkl\"))\n",
    "\n",
    "# Sauvegarde des noms des variables explicatives\n",
    "with open(os.path.join(model_data_dir, \"feature_names.txt\"), \"w\") as f:\n",
    "    f.write(\"\\n\".join(X.columns.tolist()))\n",
    "\n",
    "print(f\"Données préparées pour la modélisation sauvegardées avec succès dans le dossier : {model_data_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8 Visualisation de la distribution des données standardisées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation de la distribution des variables standardisées\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Sélection de quelques variables importantes pour la visualisation\n",
    "selected_features = ['age', 'income', 'loan_amount', 'good_price', 'loan_to_income']\n",
    "selected_indices = [X_train.columns.get_loc(feature) for feature in selected_features]\n",
    "\n",
    "for i, idx in enumerate(selected_indices):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.hist(X_train_scaled[:, idx], bins=30, alpha=0.7)\n",
    "    plt.title(f'Distribution de {X_train.columns[idx]} standardisée')\n",
    "    plt.axvline(x=0, color='r', linestyle='--')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.9 Analyse des corrélations après ingénierie des caractéristiques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un DataFrame avec les variables explicatives et la variable cible\n",
    "df_corr = pd.DataFrame(X_train, columns=X_train.columns)\n",
    "df_corr['credit_status'] = y_train.values\n",
    "\n",
    "# Calcul de la matrice de corrélation\n",
    "correlation_matrix = df_corr.corr()\n",
    "\n",
    "# Visualisation de la matrice de corrélation\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Matrice de corrélation après ingénierie des caractéristiques', fontsize=14)\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corrélations avec la variable cible\n",
    "target_correlations = correlation_matrix['credit_status'].sort_values(ascending=False)\n",
    "print(\"Corrélations avec la variable cible (credit_status) :\")\n",
    "target_correlations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.10 Résumé de la préparation pour la modélisation\n",
    "\n",
    "Dans ce notebook, nous avons effectué les opérations suivantes :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Chargement des données préparées** : Nous avons chargé les données préparées dans le notebook précédent.\n",
    "\n",
    "2. **Ingénierie des caractéristiques** : Nous avons créé de nouvelles caractéristiques qui pourraient être utiles pour la modélisation :\n",
    "   - `loan_to_income` : Ratio entre le montant du prêt et le revenu\n",
    "   - `good_price_to_loan` : Ratio entre le prix du bien et le montant du prêt\n",
    "   - `expenses_to_income` : Ratio entre les dépenses et le revenu\n",
    "\n",
    "3. **Séparation des données** : Nous avons séparé les données en variables explicatives (X) et variable cible (y), puis en ensembles d'entraînement (80%) et de test (20%).\n",
    "\n",
    "4. **Standardisation des variables numériques** : Nous avons standardisé les variables numériques pour qu'elles aient une moyenne de 0 et un écart-type de 1.\n",
    "\n",
    "5. **Sauvegarde des données préparées** : Nous avons sauvegardé les ensembles d'entraînement et de test, ainsi que le scaler, pour les utiliser dans les notebooks de modélisation.\n",
    "\n",
    "6. **Analyse des corrélations** : Nous avons analysé les corrélations entre les variables après l'ingénierie des caractéristiques pour identifier les variables les plus importantes pour la modélisation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations importantes :\n",
    "\n",
    "1. **Variables les plus corrélées avec la variable cible** :\n",
    "   - `loan_to_income` : Cette nouvelle caractéristique est fortement corrélée avec le statut de crédit, ce qui confirme l'importance du ratio entre le montant du prêt et le revenu.\n",
    "   - `loan_amount` : Le montant du prêt est positivement corrélé avec le statut de non-solvabilité.\n",
    "   - `income` : Le revenu est négativement corrélé avec le statut de non-solvabilité.\n",
    "\n",
    "2. **Distribution équilibrée** : Nous avons utilisé la stratification lors de la séparation des données pour maintenir la même distribution de la variable cible dans les ensembles d'entraînement et de test.\n",
    "\n",
    "3. **Standardisation réussie** : Les variables standardisées ont bien une moyenne proche de 0 et un écart-type proche de 1, ce qui est important pour les algorithmes sensibles à l'échelle des variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prochaine étape\n",
    "\n",
    "Dans les prochains notebooks, nous allons développer et évaluer deux modèles de classification :\n",
    "1. Régression logistique\n",
    "2. K-Nearest Neighbors (KNN)\n",
    "\n",
    "Nous comparerons ensuite leurs performances pour déterminer le meilleur modèle pour prédire le statut de crédit des clients."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
